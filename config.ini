[general]
seed=42
use_quantization=True
use_lora=True

[data_config]
personalized=False
instruct=True
generative=False
data_class=GoEmo
data_folder=data/go_emo/
responses_dirname=logged_responses/test/
train_filename=train.csv
val_filename=val.csv
test_filename=test.csv

[base_model_config]
pretrained_model_name_or_path=microsoft/phi-2
max_seq_length=2048
problem_type=multi_label_classification
attn_implementation=eager
cache_dir=models_cache

[training_config]
output_dir=checkpoints_phi-2
eval_accumulation_steps=64
per_device_train_batch_size=8
per_device_eval_batch_size=8
gradient_accumulation_steps=4
optim=paged_adamw_32bit
logging_steps=50
learning_rate=1e-4
fp16=True
max_grad_norm=1.0
num_train_epochs=10
evaluation_strategy=steps
eval_steps=0.03
warmup_ratio=0.05
group_by_length=True
save_safetensors=True
load_best_model_at_end=True
save_strategy=steps
save_steps=0.03
lr_scheduler_type=cosine
seed=42
report_to=wandb
ddp_find_unused_parameters=False

[quantization_config]
load_in_4bit=True
bnb_4bit_use_double_quant=True
bnb_4bit_quant_type=nf4
bnb_4bit_compute_dtype=torch.bfloat16

[lora_config]
r=2
lora_alpha=32
lora_dropout=0.05
bias=none
task_type=SEQ_CLS
target_modules=all