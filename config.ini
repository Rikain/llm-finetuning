[general]
use_quantization=True
use_lora=True

[paths]
dataset_folder=go_emo

[base_model_config]
base_model_name=stabilityai/stablelm-tuned-alpha-3b
max_seq_length=4096
problem_type=multi_label_classification

[training_config]
output_dir=checkpoint
per_device_train_batch_size=4
gradient_accumulation_steps=8
optim=paged_adamw_32bit
logging_steps=50
learning_rate=1e-4
fp16=True
max_grad_norm=1.0
num_train_epochs=5
evaluation_strategy=epoch
warmup_ratio=0.05
save_strategy=epoch
group_by_length=True
save_safetensors=True
lr_scheduler_type=cosine
seed=42
report_to=wandb

[quantization_config]
load_in_4bit=True
bnb_4bit_use_double_quant=True
bnb_4bit_quant_type=nf4
bnb_4bit_compute_dtype=torch.bfloat16

[lora_config]
r=2
lora_alpha=32
lora_dropout=0.05
bias=none
task_type=SEQ_CLS
target_modules=all
