[general]
seed=42
use_quantization=True
use_lora=True

[paths]
dataset_loader_folder=go_emo

[data_config]
personalized=True
instruct=False
data_folder=go_emo/data
train_filename=train.csv
val_filename=val.csv
test_filename=test.csv

[base_model_config]
base_model_name=google/flan-t5-xl
max_seq_length=1024
problem_type=multi_label_classification

[training_config]
output_dir=checkpoints_flan-t5-xl_yes_inst_yes_pers/checkpoint-10935
eval_accumulation_steps=32
per_device_train_batch_size=4
per_device_eval_batch_size=4
gradient_accumulation_steps=16
optim=paged_adamw_32bit
logging_steps=50
learning_rate=1e-4
fp16=False
max_grad_norm=1.0
num_train_epochs=5
evaluation_strategy=steps
eval_steps=0.05
warmup_ratio=0.05
group_by_length=True
save_safetensors=True
load_best_model_at_end=True
save_strategy=steps
save_steps=0.05
lr_scheduler_type=cosine
seed=42
report_to=wandb

[quantization_config]
load_in_4bit=True
bnb_4bit_use_double_quant=True
bnb_4bit_quant_type=nf4
bnb_4bit_compute_dtype=torch.bfloat16

[lora_config]
r=2
lora_alpha=32
lora_dropout=0.05
bias=none
task_type=SEQ_CLS
target_modules=all
